{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protodash Explanations for Text data\n",
    "\n",
    "In the example shown in this notebook, we train a text classifier based on [UCI SMS dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) to distinguish 'SPAM' and 'HAM' (i.e. non spam) SMS messages. We then use the ProtodashExplainer to obtain spam and ham prototypes based on the labels assigned by the text classifier. \n",
    "\n",
    "In order to run this notebook, please: \n",
    "1. Download [UCI SMS dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) dataset and place the directory 'smsspamcollection' in the location of this notebook. \n",
    "2. Place glove embedding file \"glove.6B.100d.txt\" in the location of this notebook. This can be downloaded from [here](https://nlp.stanford.edu/projects/glove/) \n",
    "3. Create 2 folders: \"results\" and \"logs\" in the location of this notebook (these are used to store training logs). \n",
    "4. The models trained in this notebook can also be accessed from [here](https://github.com/IBM/AIX360/tree/master/aix360/models/protodash) if required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Train a LSTM classifier on SMS dataset\n",
    "We train a LSTM model to label the dataset as spam / ham. The model is based on the following code: https://www.thepythoncode.com/article/build-spam-classifier-keras-python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "# import keras_metrics # for recall and precision metrics\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os.path\n",
    "from keras.models import model_from_json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 100 # the length of all sequences (number of words per sample)\n",
    "EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors\n",
    "TEST_SIZE = 0.25 # ratio of testing set\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20 # number of epochs\n",
    "\n",
    "# to convert labels to integers and vice-versa\n",
    "label2int = {\"ham\": 0, \"spam\": 1}\n",
    "int2label = {0: \"ham\", 1: \"spam\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "combined_df = pd.read_csv('SMSSpamCollection.csv', delimiter='\\t',header=None)\n",
    "combined_df.columns = ['label', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text and store as a column in original df\n",
    "X = combined_df['text'].values.tolist()\n",
    "y = combined_df['label'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text tokenization\n",
    "# vectorizing text, turning each text into sequence of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "# convert to sequence of integers\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "# pad sequences at the beginning of each sequence with 0's\n",
    "# for example if SEQUENCE_LENGTH=4:\n",
    "# [[5, 3, 2], [5, 1, 2, 3], [3, 4]]\n",
    "# will be transformed to:\n",
    "# [[0, 5, 3, 2], [5, 1, 2, 3], [0, 0, 3, 4]]\n",
    "X = pad_sequences(X, maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [ label2int[label] for label in y ]\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and shuffle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vectors(tokenizer, dim=100):\n",
    "    embedding_index = {}\n",
    "    with open(f\"glove.6B.{dim}d.txt\", encoding='utf8') as f:\n",
    "        for line in tqdm.tqdm(f, \"Reading GloVe\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vectors = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_index[word] = vectors\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found will be 0s\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(tokenizer, lstm_units):\n",
    "    \"\"\"\n",
    "    Constructs the model,\n",
    "    Embedding vectors => LSTM => 2 output Fully-Connected neurons with softmax activation\n",
    "    \"\"\"\n",
    "    # get the GloVe embedding vectors\n",
    "    embedding_matrix = get_embedding_vectors(tokenizer)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(tokenizer.word_index)+1,\n",
    "              EMBEDDING_SIZE,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              input_length=SEQUENCE_LENGTH))\n",
    "\n",
    "    model.add(LSTM(lstm_units, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    # compile as rmsprop optimizer\n",
    "    # aswell as with recall metric\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading GloVe: 400001it [00:07, 55836.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 100)          901000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,018,506\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 901,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# constructs the model with 128 LSTM units\n",
    "model = get_model(tokenizer=tokenizer, lstm_units=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model or load trained model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sms-lstm-forprotodash.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9j/zsyct9fn7ts_r5fp613fn3dw0000gn/T/ipykernel_60775/2181009971.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# load json and create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sms-lstm-forprotodash.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mloaded_model_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sms-lstm-forprotodash.json'"
     ]
    }
   ],
   "source": [
    "to_train = False\n",
    "\n",
    "if (to_train): \n",
    "\n",
    "    # initialize our ModelCheckpoint and TensorBoard callbacks\n",
    "    # model checkpoint for saving best weights\n",
    "    model_checkpoint = ModelCheckpoint(\"results/spam_classifier_{val_loss:.2f}\", save_best_only=True,\n",
    "                                        verbose=1)\n",
    "    # for better visualization\n",
    "    tensorboard = TensorBoard(f\"logs/spam_classifier_{time.time()}\")\n",
    "    # print our data shapes\n",
    "    print(\"X_train.shape:\", X_train.shape)\n",
    "    print(\"X_test.shape:\", X_test.shape)\n",
    "    print(\"y_train.shape:\", y_train.shape)\n",
    "    print(\"y_test.shape:\", y_test.shape)\n",
    "    # train the model\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "              batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "              callbacks=[tensorboard, model_checkpoint],\n",
    "              verbose=1)\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"sms-lstm-forprotodash.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"sms-lstm-forprotodash.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "        \n",
    "else: \n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open(\"sms-lstm-forprotodash.json\", 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(loaded_model_json)\n",
    "\n",
    "    # load weights into new model\n",
    "    model.load_weights(\"sms-lstm-forprotodash.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    # print model \n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 1s 13ms/step - loss: 0.7349 - accuracy: 0.3302 - precision: 0.3297 - recall: 0.3295\n",
      "[+] Accuracy: 33.02%\n",
      "[+] Precision:   32.97%\n",
      "[+] Recall:   32.95%\n"
     ]
    }
   ],
   "source": [
    "# get the loss and metrics\n",
    "result = model.evaluate(X_test, y_test)\n",
    "# extract those\n",
    "loss = result[0]\n",
    "accuracy = result[1]\n",
    "precision = result[2]\n",
    "recall = result[3]\n",
    "\n",
    "print(f\"[+] Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"[+] Precision:   {precision*100:.2f}%\")\n",
    "print(f\"[+] Recall:   {recall*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Get model predictions for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(doclist):\n",
    "    \n",
    "    sequence = tokenizer.texts_to_sequences(doclist)\n",
    "    \n",
    "    # pad the sequence\n",
    "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "    # get the prediction as one-hot encoded vector\n",
    "    prediction = model.predict(sequence)\n",
    "    \n",
    "    return (prediction)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text = \"Congratulations! you have won 100,000$ this week, click here to claim fast\"\n",
    "pred = get_predictions([text])\n",
    "print(int2label [ np.argmax(pred, axis=1)[0] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi man, I was wondering if we can meet tomorrow.\"\n",
    "pred = get_predictions([text])\n",
    "print(int2label [ np.argmax(pred, axis=1)[0] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doclist = combined_df['text'].values.tolist()\n",
    "one_hot_prediction = get_predictions(doclist)\n",
    "label_prediction = np.argmax(one_hot_prediction, axis=1)\n",
    "\n",
    "# 0: ham, 1:spam\n",
    "idx_ham = (label_prediction == 0)\n",
    "idx_spam = (label_prediction == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 3. Use protodash explainer to compute spam and ham prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from aix360.algorithms.protodash import ProtodashExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to vectors using TF-IDF for use in explainer\n",
    "\n",
    "We use TF-IDF vectors for scalability reasons as the original embedding vector for a full sentence can be quite large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 8713)\n"
     ]
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(doclist)\n",
    "\n",
    "vec = vectorizer.transform(doclist)\n",
    "docvec = vec.toarray()\n",
    "print(docvec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate spam and ham messages and corrsponding vectors\n",
    "\n",
    "docvec_spam = docvec[idx_spam, :]\n",
    "docvec_ham = docvec[idx_ham, :]\n",
    "\n",
    "df_spam = combined_df[idx_spam]['text']\n",
    "df_ham = combined_df[idx_ham]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4103,)\n",
      "(1469,)\n"
     ]
    }
   ],
   "source": [
    "print(df_spam.shape)\n",
    "print(df_ham.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute prototypes for spam and ham datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = ProtodashExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "\n",
    "# call protodash explainer\n",
    "# S contains indices of the selected prototypes\n",
    "# W contains importance weights associated with the selected prototypes \n",
    "(W_spam, S_spam, _) = explainer.explain(docvec_spam, docvec_spam, m=m)\n",
    "(W_ham, S_ham, _) = explainer.explain(docvec_ham, docvec_ham, m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prototypes from index\n",
    "df_spam_prototypes = df_spam.iloc[S_spam].copy()\n",
    "df_ham_prototypes = df_ham.iloc[S_ham].copy()\n",
    "\n",
    "#normalize weights\n",
    "W_spam = np.around(W_spam/np.sum(W_spam), 2) \n",
    "W_ham = np.around(W_ham/np.sum(W_ham), 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAM prototypes with weights:\n",
      "----------------------------\n",
      "0.14 The last thing i ever wanted to do was hurt you. And i didn't think it would have. You'd laugh, be embarassed, delete the tag and keep going. But as far as i knew, it wasn't even up. The fact that you even felt like i would do it to hurt you shows you really don't know me at all. It was messy wednesday, but it wasn't bad. The problem i have with it is you HAVE the time to clean it, but you choose not to. You skype, you take pictures, you sleep, you want to go out. I don't mind a few things here and there, but when you don't make the bed, when you throw laundry on top of it, when i can't have a friend in the house because i'm embarassed that there's underwear and bras strewn on the bed, pillows on the floor, that's something else. You used to be good about at least making the bed.\n",
      "0.1 I can't believe how attached I am to seeing you every day. I know you will do the best you can to get to me babe. I will go to teach my class at your midnight\n",
      "0.09 What do u want when i come back?.a beautiful necklace as a token of my heart for you.thats what i will give but ONLY to MY WIFE OF MY LIKING.BE THAT AND SEE..NO ONE can give you that.dont call me.i will wait till i come.\n",
      "0.1 Sorry,  in meeting I'll call you later\n",
      "0.1 I had been hoping i would not have to send you this message. My rent is due and i dont have enough for it. My reserves are completely gone. Its a loan i need and was hoping you could her. The balance is  &lt;#&gt; . Is there a way i could get that from you, till mid march when i hope to pay back.\n",
      "0.1 Ok then no need to tell me anything i am going to sleep good night\n",
      "0.1 Are your freezing ? Are you home yet ? Will you remember to kiss your mom in the morning? Do you love me ? Do you think of me ? Are you missing me yet ?\n",
      "0.09 Unfortunately i've just found out that we have to pick my sister up from the airport that evening so don't think i'll be going out at all. We should try to go out one of th\n",
      "0.09 07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow\n",
      "0.09 hey, looks like I was wrong and one of the kappa guys numbers is still on my phone, if you want I can text him and see if he's around\n"
     ]
    }
   ],
   "source": [
    "print(\"SPAM prototypes with weights:\")\n",
    "print(\"----------------------------\")\n",
    "for i in range(m):\n",
    "    print(W_spam[i], df_spam_prototypes.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAM prototypes with weights:\n",
      "----------------------------\n",
      "0.09 Only if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok.\n",
      "0.12 I can't keep going through this. It was never my intention to run you out, but if you choose to do that rather than keep the room clean so *I* don't have to say no to visitors, then maybe that's the best choice. Yes, I wanted you to be embarassed, so maybe you'd feel for once how I feel when i have a friend who wants to drop buy and i have to say no, as happened this morning. I've tried everything. I don't know what else to do.\n",
      "0.09 I'm coming back on Thursday. Yay. Is it gonna be ok to get the money. Cheers. Oh yeah and how are you. Everything alright. Hows school. Or do you call it work now\n",
      "0.11 I was wondering if it would be okay for you to call uncle john and let him know that things are not the same in nigeria as they r here. That  &lt;#&gt;  dollars is 2years sent and that you know its a strain but i plan to pay back every dime he gives. Every dime so for me to expect anything from you is not practical. Something like that.\n",
      "0.09 So anyways, you can just go to your gym or whatever, my love *smiles* I hope your ok and having a good day babe ... I miss you so much already\n",
      "0.1 I am not at all happy with what you saying or doing\n",
      "0.1 Dunno dat's wat he told me. Ok lor...\n",
      "0.11 I'm always on yahoo messenger now. Just send the message to me and i.ll get it you may have to send it in the mobile mode sha but i.ll get it. And will reply.\n",
      "0.1 Dear how you. Are you ok?\n",
      "0.1 Dont hesitate. You know this is the second time she has had weakness like that. So keep i notebook of what she eat and did the day before or if anything changed the day before so that we can be sure its nothing\n"
     ]
    }
   ],
   "source": [
    "print(\"HAM prototypes with weights:\")\n",
    "print(\"----------------------------\")\n",
    "for i in range(m):\n",
    "    print(W_ham[i], df_ham_prototypes.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a message, look for similar messages that are classified as spam by classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "sample_text = df_spam.iloc[k]\n",
    "sample_vec = docvec_spam[k]\n",
    "sample_vec = sample_vec.reshape(1, sample_vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n",
      "(1, 8713)\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)\n",
    "print(sample_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvec_spam_other = docvec_spam[np.arange(docvec_spam.shape[0]) != k, :] \n",
    "df_spam_other = df_spam.iloc[np.arange(docvec_spam.shape[0]) != k].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample spam text and find samples similar to it. \n",
    "(W1_spam, S1_spam, _) = explainer.explain(sample_vec, docvec_spam_other, m=m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize weights\n",
    "W1_spam = np.around(W1_spam/np.sum(W1_spam), 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2848, 2747,  283, 1061, 3350, 2134, 3656, 1519,  435, 3907])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text\n",
      "-------------\n",
      "U dun say so early hor... U c already then say...\n",
      "\n",
      "Similar SPAM prototypes:\n",
      "------------------------\n",
      "0.16 Okie but i scared u say i fat... Then u dun wan me already...\n",
      "0.13 Why are u up so early?\n",
      "0.11 Can you say what happen\n",
      "0.12 say thanks2. \n",
      "0.1 Sun ah... Thk mayb can if dun have anythin on... Thk have to book e lesson... E pilates is at orchard mrt u noe hor...  \n",
      "0.09 Nothing lor... A bit bored too... Then y dun u go home early 2 sleep today...\n",
      "0.08 Aiyo u so poor thing... Then u dun wan 2 eat? U bathe already?\n",
      "0.07 Who's there say hi to our drugdealer\n",
      "0.07 Huh so early.. Then ü having dinner outside izzit?\n",
      "0.06 Here got ur favorite oyster... N got my favorite sashimi... Ok lar i dun say already... Wait ur stomach start rumbling...\n"
     ]
    }
   ],
   "source": [
    "# similar spam prototypes\n",
    "print(\"original text\")\n",
    "print(\"-------------\")\n",
    "print(sample_text)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Similar SPAM prototypes:\")\n",
    "print(\"------------------------\")\n",
    "m = 10\n",
    "for i in range(m):\n",
    "    print(W1_spam[i], df_spam_other.iloc[S1_spam[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "Note several spam messages repeat in the dataset as these may have been sent by the same entity to multiple users. As a consequence, the explainer retireves these. Try with a different k above to see prototypes corrsponding to other sample messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a ham message, look for similar messages that are classified as spam by classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "sample_text = df_ham.iloc[k]\n",
    "sample_vec = docvec_ham[k]\n",
    "sample_vec = sample_vec.reshape(1, sample_vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\n",
      "(1, 8713)\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)\n",
    "print(sample_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "docvec_ham_other = docvec_ham[np.arange(docvec_ham.shape[0]) != k, :] \n",
    "df_ham_other = df_ham.iloc[np.arange(docvec_ham.shape[0]) != k].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample spam text and find samples similar to it. \n",
    "(W1_ham, S1_ham, _) = explainer.explain(sample_vec, docvec_ham_other, m=m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize weights\n",
    "W1_ham = np.around(W1_ham/np.sum(W1_ham), 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1394,   27,  367,  436,  671,  695,  898, 1030,  754, 1420])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text\n",
      "-------------\n",
      "URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\n",
      "\n",
      "Similar HAM prototypes:\n",
      "------------------------\n",
      "1.0 URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\n",
      "-0.0 Just so that you know,yetunde hasn't sent money yet. I just sent her a text not to bother sending. So its over, you dont have to involve yourself in anything. I shouldn't have imposed anything on you in the first place so for that, i apologise.\n",
      "-0.0 500 New Mobiles from 2004, MUST GO! Txt: NOKIA to No: 89545 & collect yours today!From ONLY £1 www.4-tc.biz 2optout 087187262701.50gbp/mtmsg18 TXTAUCTION\n",
      "-0.0 For ur chance to win a £250 cash every wk TXT: ACTION to 80608. T's&C's www.movietrivia.tv custcare 08712405022, 1x150p/wk\n",
      "0.0 Only if you promise your getting out as SOON as you can. And you'll text me in the morning to let me know you made it in ok.\n",
      "-0.0 FreeMSG You have been awarded a FREE mini DIGITAL CAMERA, just reply SNAP to collect your prize! (quizclub Opt out? Stop 80122300p/wk SP:RWM Ph:08704050406)\n",
      "-0.0 No we put party 7 days a week and study lightly, I think we need to draw in some custom checkboxes so they know we're hardcore\n",
      "-0.0 tells u 2 call 09066358152 to claim £5000 prize. U have 2 enter all ur mobile & personal details @ the prompts. Careful!\n",
      "0.0 For ur chance to win a £250 cash every wk TXT: ACTION to 80608. T's&C's www.movietrivia.tv custcare 08712405022, 1x150p/wk.\n",
      "0.0 Free entry to the gr8prizes wkly comp 4 a chance to win the latest Nokia 8800, PSP or £250 cash every wk.TXT GREAT to 80878 http//www.gr8prizes.com 08715705022\n"
     ]
    }
   ],
   "source": [
    "# similar spam prototypes\n",
    "print(\"original text\")\n",
    "print(\"-------------\")\n",
    "print(sample_text)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Similar HAM prototypes:\")\n",
    "print(\"------------------------\")\n",
    "m = 10\n",
    "for i in range(m):\n",
    "    print(W1_ham[i], df_ham_other.iloc[S1_ham[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
